# AI-Studio-飞桨常规赛:论文引用网络节点分类

## 项目描述
图神经网络（Graph Neural Network）是一种专门处理图结构数据的神经网络，目前被广泛应用于推荐系统、金融风控、生物计算等领域。图神经网络的经典问题主要有三类，分别为节点分类、连接预测和图分类。本次比赛旨在让参赛同学了解并掌握如何使用图神经网络处理节点分类问题。

在过去的一个世纪里，科学出版物的数量每12年增加近一倍，对每一种出版物的主题及领域进行自动分类已成为当下十分重要的工作。本次任务的目标是预测未知论文的主题类别，如软件工程，人工智能，语言计算和操作系统等。比赛所选35个领域标签已得到论文作者和arXiv版主确认并标记。

本次比赛选用的数据集为arXiv论文引用网络——ogbn-arixv数据集的子集。ogbn-arixv数据集由大量的学术论文组成，论文之间的引用关系形成一张巨大的有向图，每一条有向边表示一篇论文引用另一篇论文，每一个节点提供100维简单的词向量作为节点特征。在论文引用网络中，我们已对训练集对应节点做了论文类别标注处理。本次任务希望参赛者通过已有的节点类别以及论文之间的引用关系，预测未知节点的论文类别。

## 数据内容
1.学术网络图数据： 该图包含1647958条有向边，130644个节点，参赛者报名成功后即可通过比赛数据集页面提供edges.csv以及feat.npy下载并读取数据。图上的每个节点代表一篇论文，论文从0开始编号；图上的每一条边包含两个编号，例如 3，4代表第3篇论文引用了第4篇论文。图构造可以参照AiStudio上提供的基线系统项目了解数据读取方法。

2.训练集与测试集： 训练集的标注数据有70235条，测试集的标注数据有37311条。训练数据给定了论文编号与类别，如3，15 代表编号为3的论文类别为15。测试集数据只提供论文编号，不提供论文类别，需要参赛者预测其类别。

具体数据介绍： 1.图数据： feat.npy为Numpy格式存储的节点特征矩阵，Shape为(130644, 100)，即130644个节点，每个节点含100维特征。其中 feat.npy可以用numpy.load(“feat.npy”)读取。edges.csv用于标记论文引用关系，为无向图，且由两列组成，没有表头。

## 思路
通过借鉴UniMP模型，来处理该问题。
有关Unimp的具体详细信息可以点击[源代码](https://github.com/PaddlePaddle/PGL/blob/main/ogb_examples/nodeproppred/unimp/)查看

UniMP：统一消息传递模型

在半监督图节点分类场景下，节点之间通过边相连接，部分节点被打上标签。任务要求模型通过监督学习的方式，拟合被标注节点数据，并对未标注的节点进行预测。如下图所示，在一般机器学习的问题上，已标注的训练数据在新数据的推断上，并不能发挥直接的作用，因为数据的输入是独立的。然而在图神经网络的场景下，已有的标注数据可以从节点与节点的连接中，根据图结构关系推广到新的未标注数据中。


一般应用于半监督节点分类的算法分为图神经网络和标签传递算法两类，它们都是通过消息传递的方式（前者传递特征、后者传递标签）进行节点标签的学习和预测。其中经典标签传递算法如LPA，只考虑了将标签在图上进行传递，而图神经网络算法大多也只是使用了节点特征以及图的链接信息进行分类。但是单纯考虑标签传递或者节点特征都是不足够的。

![image](https://ai-studio-static-online.cdn.bcebos.com/d8e3d63856de4609a88b804a8eeb1004f5745b84c8634d2b9e034f67fdd8d964)

统一消息传递模型 UniMP，将上述两种消息统一到框架中，同时实现了节点的特征与标签传递，显著提升了模型的泛化效果。UniMP以Graph Transformer模型作为基础骨架，联合使用标签嵌入方法，将节点特征和部分节点标签同时输入至模型中，从而实现了节点特征和标签的同时传递。

![image](https://ai-studio-static-online.cdn.bcebos.com/c899bb03c6c74e29b6d80a946502b297d0caec7c3be442ceafae45c7762344d8)

## 神经网络层设计

Tranformer在NLP自然语言处理中有着巨大的作用，可以将其multi-head attention的机制运用到图学习中.
计算的公式如下图所示
![image](https://ai-studio-static-online.cdn.bcebos.com/b67a267753ac47ab94b6237a15f3fcbf6cc37f3f8ed74f68abc18bdc72b8c63e)

<q,k>表示指数项的点积函数$<q, k> = e^{\frac{q^Tk}{\sqrt{d}}}$，d为隐藏层尺寸。对于第 l 层的第 c 个注意力，通过可训练参数$W^{(l)}{c,q}$, $W^{(l)}{c,k}$, $b^{(l)}{c,q}$, $b^{(l)}{c,k}$ 将srcfeature($h^{(l)}i$)和dstfeature($h^{(l)}_j$)转换成query vector和key vector，若有edgefeature，作为额外信息再加入key vector中，最后通过上式点积函数计算multi-head attention

接下来是消息聚合的公式
![image](https://ai-studio-static-online.cdn.bcebos.com/b774e5d22e534ee1b51f100f3f76c597ba01510b3aa24f869d723c081417aafc)

$||$ 是将C个注意力拼接的操作。公式中用多头注意力矩阵替换了原先的归一化邻接矩阵来进行消息传递, $r_i$ 和 $β_i$ 为加入的gated residual connections的参数，目的是为了避免过度平滑。

![image](https://ai-studio-static-online.cdn.bcebos.com/21cd918e8f5e4081a865bd8670eda8e261a06c40cb3a49be91dabbc0b290e912)

每层最后的输出是通过计算平均值做为下一层训练的特征

## 项目结构
```
-|data
-|work
-README.MD
-xxx.ipynb
```
## 使用方式
A：在AI Studio上[运行本项目](https://aistudio.baidu.com/aistudio/projectdetail/1414886)
B：此处由项目作者进行撰写使用方式。
